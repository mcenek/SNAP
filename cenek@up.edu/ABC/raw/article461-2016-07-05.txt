
A robot army won't look like what we have come to expect from the movies
A robot army could never be held morally responsible for committing war crimes, says an Australian ethicist, and so any wars they fight could be considered unjust.
Dr Robert Sparrow of Monash University, who specialises in the ethics of new technologies, will lay out his argument in the Journal of Applied Philosophy.
"To fight a war properly you must always be able to identify somebody who is responsible for the deaths that ensue," says Sparrow, who is also with the Centre for Applied Philosophy and Public Ethics at the University of Melbourne.
"As this condition cannot be met in relation to deaths caused by an autonomous weapon system it would therefore be unethical to deploy such systems in warfare."
Sparrow's article was triggered by his discovery that the military is the largest funder of robotics research. 
For example, he says, the US military is developing an army that places autonomous machines in key roles on the front line.
Currently, the army uses semi-autonomous robots in mine clearing and bomb disposal. And for any machine that kills people, there is always a human being that can be held morally responsible, says Sparrow.
He says this applies, for example, to unmanned combat aerial vehicles that are programmed to help locate specific targets and fire on them, currently used in the Middle East conflict.
But, asks Sparrow, what would happen if machines themselves were given the decision about who to kill?
Autonomous killing machines
Having killer robots on the front line instead of young men and women may be more politically acceptable, but will this mean any war crimes could be blamed on the machines, asks Sparrow?
